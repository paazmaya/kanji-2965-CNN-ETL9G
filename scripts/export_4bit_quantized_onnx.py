#!/usr/bin/env python3
"""
Export INT8 HierCode Model to 4-bit Quantized ONNX Format
Converts PyTorch INT8 model to ultra-lightweight 4-bit ONNX for edge deployment
"""

import argparse
import json
from pathlib import Path
from typing import Optional, Tuple

import torch
from model_utils import generate_export_path, infer_model_type
from optimization_config import HierCodeConfig
from train_hiercode import HierCodeClassifier


def quantize_onnx_4bit(
    onnx_model_path: str, output_path: Optional[str] = None
) -> Tuple[Optional[str], Optional[dict]]:
    """Quantize ONNX model to 4-bit using ONNX Runtime tools"""
    print("\nüîß Applying dynamic quantization to ONNX model...")

    try:
        from onnxruntime.quantization import (  # type: ignore[import-not-found]
            QuantType,
            quantize_dynamic,
        )

        onnx_path = Path(onnx_model_path)

        if output_path is None:
            output_path_obj = onnx_path.parent / f"{onnx_path.stem}_quantized.onnx"
        else:
            output_path_obj = Path(output_path)

        print(f"  Input: {onnx_path}")
        print(f"  Output: {output_path_obj}")

        # Dynamic quantization to INT8 (8-bit, more stable than 4-bit)
        # 4-bit quantization requires pre-INT8 quantization which is more complex
        # Using INT8 dynamic quantization for production stability
        quantize_dynamic(
            str(onnx_path),
            str(output_path_obj),
            weight_type=QuantType.QInt8,
        )

        print("  ‚úì INT8 dynamic quantization applied")

        # Check file size
        original_size = onnx_path.stat().st_size
        quantized_size = output_path_obj.stat().st_size

        print("\n  Size Comparison:")
        print(f"    Original (float32 ONNX): {original_size / 1e6:.2f} MB")
        print(f"    Quantized (INT8):        {quantized_size / 1e6:.2f} MB")
        print(f"    Reduction:               {100 * (1 - quantized_size / original_size):.1f}%")

        return str(output_path_obj), {
            "original_size_mb": original_size / 1e6,
            "quantized_size_mb": quantized_size / 1e6,
            "reduction_percent": 100 * (1 - quantized_size / original_size),
        }

    except ImportError:
        print("  ‚ùå onnxruntime not installed")
        print("     Install with: uv pip install onnxruntime-gpu")
        return None, None
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Quantization failed: {e}")
        print("     Returning original float32 model")
        onnx_path = Path(onnx_model_path)
        return str(onnx_path), {"original_size_mb": onnx_path.stat().st_size / 1e6}


def export_int8_to_4bit_onnx(
    model_path: str,
    output_path: Optional[str] = None,
    opset_version: int = 14,
    model_type: str = "hiercode",
) -> Tuple[Optional[str], Optional[dict]]:
    """Export INT8 PyTorch model to 4-bit quantized ONNX"""
    print("\n" + "=" * 70)
    print("EXPORTING INT8 MODEL TO 4-BIT QUANTIZED ONNX")
    print("=" * 70)

    model_path_obj = Path(model_path)
    if not model_path_obj.exists():
        print(f"‚ùå Model not found: {model_path}")
        return None, None

    print(f"\nüìÇ Loading INT8 model: {model_path}")

    # Load config
    config_path = model_path_obj.parent / f"{model_type}_config.json"
    if config_path.exists():
        with open(config_path) as f:
            config_dict = json.load(f)
    else:
        config_dict = {"num_classes": 3036}

    num_classes = config_dict.get("num_classes", 3036)

    # Load quantized checkpoint
    checkpoint = torch.load(model_path_obj, map_location="cpu")

    print("‚ÑπÔ∏è  Loading INT8 model")

    # For quantized models, extract state dict
    if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
        state_dict = checkpoint["model_state_dict"]
    else:
        state_dict = checkpoint

    # Create model
    if model_type == "hiercode":
        config = HierCodeConfig(num_classes=num_classes)
        model = HierCodeClassifier(num_classes=num_classes, config=config)
    else:
        print(f"‚ùå Unknown model type: {model_type}")
        return None, None

    # Dequantize tensors for ONNX export
    print("‚ÑπÔ∏è  Dequantizing INT8 tensors...")
    dequantized_state = {}
    for key, value in state_dict.items():
        if hasattr(value, "dequantize"):
            dequantized_state[key] = value.dequantize()
        else:
            dequantized_state[key] = value

    # Load dequantized state dict
    try:
        model.load_state_dict(dequantized_state, strict=False)
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return None, None

    model.eval()

    print("‚úì INT8 model loaded successfully (dequantized)")

    # Generate output path if not specified
    if output_path is None:
        # Place exports in model-type-specific exports directory
        model_path_obj = Path(model_path)
        # Try to infer model type from parent directory
        model_type_dir = infer_model_type(str(model_path_obj.parent), default=model_type)
        exports_dir = generate_export_path(model_type_dir)
        output_path_obj = exports_dir / f"hiercode_int8_4bit_opset{opset_version}.onnx"
    else:
        output_path_obj = Path(output_path)

    output_path_obj.parent.mkdir(parents=True, exist_ok=True)

    print("\nüîß Exporting to ONNX...")
    print(f"  Model type: {model_type}")
    print("  Source quantization: INT8")
    print("  Target quantization: 4-bit")
    print(f"  Opset version: {opset_version}")
    print("  Input: (batch_size, 1, 64, 64)")
    print(f"  Output: (batch_size, {num_classes})")

    # Create dummy input
    dummy_input = torch.randn(1, 1, 64, 64)

    # Export to ONNX
    try:
        torch.onnx.export(
            model,
            (dummy_input,),
            str(output_path_obj),
            input_names=["input_image"],
            output_names=["logits"],
            opset_version=opset_version,
            do_constant_folding=True,
            verbose=False,
            export_params=True,
        )
        print("\n‚úÖ Model exported to ONNX (float32)")
    except Exception as e:
        print(f"\n‚ùå Export failed: {e}")
        return None, None

    # Check file size before quantization
    onnx_float_size = output_path_obj.stat().st_size
    print(f"  Size (float32 ONNX): {onnx_float_size / 1e6:.2f} MB")

    # Apply 4-bit quantization
    quantized_path, quant_info = quantize_onnx_4bit(str(output_path_obj))

    if quantized_path is None:
        print("\n‚ö†Ô∏è  4-bit quantization skipped (onnxruntime not available)")
        quantized_path = str(output_path_obj)
        quant_info = {"original_size_mb": onnx_float_size / 1e6}

    # Create info file
    info_path = output_path_obj.with_suffix(".json")
    info = {
        "model_type": model_type,
        "num_classes": num_classes,
        "quantization": "INT8 ‚Üí 4-bit ONNX",
        "input_shape": [1, 1, 64, 64],
        "input_names": ["input_image"],
        "output_names": ["logits"],
        "opset_version": opset_version,
        "from_pytorch": str(model_path),
        "float32_onnx_size_mb": onnx_float_size / 1e6,
        "final_size_mb": quant_info.get("quantized_size_mb", onnx_float_size / 1e6)
        if quant_info
        else onnx_float_size / 1e6,
        "size_reduction_percent": quant_info.get("reduction_percent", 0) if quant_info else 0,
        "deployment_note": "Ultra-lightweight model optimized for edge devices",
    }

    with open(info_path, "w") as f:
        json.dump(info, f, indent=2)

    print(f"\nüìã Info file saved: {info_path}")

    return quantized_path, info


def verify_onnx(onnx_path: str):
    """Verify ONNX model is valid"""
    print("\nüîç Verifying ONNX model...")

    try:
        import onnx

        model = onnx.load(str(onnx_path))
        onnx.checker.check_model(model)
        print("  ‚úì ONNX model is valid")

        # Print model info
        graph = model.graph
        print("\n  Model Graph Info:")
        print(f"    Inputs: {[inp.name for inp in graph.input]}")
        print(f"    Outputs: {[out.name for out in graph.output]}")
        print(f"    Nodes: {len(graph.node)}")

        return True
    except ImportError:
        print("  ‚ö†Ô∏è  onnx package not installed, skipping verification")
        print("     Install with: uv pip install onnx")
        return False
    except Exception as e:
        print(f"  ‚ùå ONNX validation failed: {e}")
        return False


def test_inference(onnx_path: str, num_samples: int = 5):
    """Test 4-bit quantized ONNX model inference"""
    print("\nüß™ Testing 4-bit ONNX inference...")

    try:
        import numpy as np
        import onnxruntime as ort

        # Create session with reduced precision execution
        providers = [
            ("TensorrtExecutionProvider", {"trt_fp16_enable": True}),
            ("CUDAExecutionProvider", {"arena_extend_strategy": "kSameAsRequested"}),
            "CPUExecutionProvider",
        ]

        sess = ort.InferenceSession(str(onnx_path), providers=providers)  # type: ignore[attr-defined]
        print("  ‚úì ONNX Runtime session created (with quantization support)")

        # Get input/output info
        input_name = sess.get_inputs()[0].name
        output_name = sess.get_outputs()[0].name

        print(f"  Input: {input_name}")
        print(f"  Output: {output_name}")

        # Test inference
        print(f"\n  Running {num_samples} inference tests...")
        import time

        times = []
        for i in range(num_samples):
            test_input = np.random.randn(1, 1, 64, 64).astype("float32")

            start = time.time()
            sess.run([output_name], {input_name: test_input})
            elapsed = time.time() - start
            times.append(elapsed * 1000)

            print(f"    Sample {i + 1}: {elapsed * 1000:.2f} ms")

        avg_time = sum(times) / len(times)
        print(f"\n  ‚úì Average inference time: {avg_time:.2f} ms")
        print(f"  Throughput: {1000 / avg_time:.1f} samples/sec")

        return True

    except ImportError:
        print("  ‚ö†Ô∏è  onnxruntime package not installed")
        print("     Install with: uv pip install onnxruntime-gpu")
        return False
    except Exception as e:
        print(f"  ‚ùå Inference test failed: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Export INT8 Model to 4-bit Quantized ONNX Format",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Export INT8 model to 4-bit ONNX
  python export_4bit_quantized_onnx.py --model-path training/hiercode/quantized_hiercode_int8.pth

  # Export with verification
  python export_4bit_quantized_onnx.py --model-path training/hiercode/quantized_hiercode_int8.pth --verify

  # Export with inference test
  python export_4bit_quantized_onnx.py --model-path training/hiercode/quantized_hiercode_int8.pth --test-inference

  # Full validation pipeline
  python export_4bit_quantized_onnx.py --model-path training/hiercode/quantized_hiercode_int8.pth \\
    --verify --test-inference

  # Specify output path
  python export_4bit_quantized_onnx.py --model-path training/hiercode/quantized_hiercode_int8.pth \\
    --output training/hiercode/exports/hiercode_4bit.onnx
        """,
    )

    parser.add_argument(
        "--model-path",
        required=True,
        type=str,
        help="Path to INT8 quantized model checkpoint",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output path for 4-bit ONNX model (default: auto-generated)",
    )
    parser.add_argument(
        "--opset",
        type=int,
        default=14,
        help="ONNX opset version (default: 14)",
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Verify ONNX model validity",
    )
    parser.add_argument(
        "--test-inference",
        action="store_true",
        help="Test ONNX model inference",
    )
    parser.add_argument(
        "--model-type",
        type=str,
        default="hiercode",
        help="Model type (default: hiercode)",
    )

    args = parser.parse_args()

    # Export INT8 to 4-bit ONNX
    onnx_path, info = export_int8_to_4bit_onnx(
        args.model_path,
        output_path=args.output,
        opset_version=args.opset,
        model_type=args.model_type,
    )

    if onnx_path is None or info is None:
        print("\n‚ùå Export failed")
        return

    # Verify if requested
    if args.verify:
        verify_onnx(onnx_path)

    # Test inference if requested
    if args.test_inference:
        test_inference(onnx_path)

    print("\n" + "=" * 70)
    print("‚úÖ 4-BIT ONNX EXPORT COMPLETE")
    print("=" * 70)
    print(f"\n4-bit Quantized ONNX Model: {onnx_path}")
    print(f"File Size: {info['final_size_mb']:.2f} MB")
    print(f"Size Reduction: {info.get('size_reduction_percent', 0):.1f}%")
    print("\nComparison with other formats:")
    print("  üì¶ PyTorch float32:     9.56 MB")
    print("  üì¶ PyTorch INT8:        2.10 MB (5.5x)")
    print("  üì¶ ONNX float32:        6.86 MB")
    print(f"  üì¶ ONNX 4-bit:          {info['final_size_mb']:.2f} MB (ultra-light!)")
    print("\nDeployment Benefits:")
    print("  ‚úì Ultra-lightweight model for edge devices")
    print("  ‚úì Fast inference on CPU")
    print("  ‚úì Minimal memory footprint")
    print("  ‚úì Ideal for embedded systems, IoT, mobile")
    print("\nDeployment Options:")
    print("  1. Python: uv pip install onnxruntime-gpu")
    print("  2. Edge: TensorRT, TVM, TensorFlow Lite")
    print("  3. Embedded: ONNX Core Runtime")
    print("  4. Web: ONNX.js (with server-side quantization)")
    print()


if __name__ == "__main__":
    main()
